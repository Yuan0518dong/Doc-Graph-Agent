import re
import uuid
import json
from dataclasses import dataclass, asdict
from typing import List, Dict
from pathlib import Path

# === è·¯å¾„é…ç½® ===
BASE_DIR = Path(__file__).resolve().parent.parent.parent
INPUT_DIR = BASE_DIR / "data" / "processed"
OUTPUT_FILE = BASE_DIR / "data" / "processed" / "hierarchical_chunks.jsonl"


@dataclass
class HeaderNode:
    level: int
    title: str


@dataclass
class MarkdownChunk:
    id: str  # å”¯ä¸€æ ‡è¯†ç¬¦
    content: str  # æ–‡æœ¬å†…å®¹
    metadata: Dict  # åŒ…å« path, source, headers(å…³é”®)


class MarkdownContextSplitter:
    def __init__(self):
        # æ­£åˆ™ï¼šåŒ¹é… Markdown æ ‡é¢˜ (å¦‚ "## 1.1 èƒŒæ™¯")
        self.header_pattern = re.compile(r'^(#{1,6})\s+(.*)')

    def split_text(self, text: str, source_name: str = "unknown") -> List[Dict]:
        lines = text.split('\n')
        chunks = []

        # æ ¸å¿ƒæ•°æ®ç»“æ„ï¼šæ ˆ
        header_stack: List[HeaderNode] = []
        content_buffer: List[str] = []

        for line in lines:
            match = self.header_pattern.match(line)

            if match:
                # 1. é‡åˆ°æ–°æ ‡é¢˜ -> ç»“ç®—ä¸Šä¸€æ®µå†…å®¹
                if content_buffer:
                    self._save_chunk(chunks, header_stack, content_buffer, source_name)
                    content_buffer = []  # æ¸…ç©º

                # 2. æ ˆæ“ä½œ (ç»´æŠ¤å±‚çº§)
                new_level = len(match.group(1))
                new_title = match.group(2).strip()

                # Pop: å¦‚æœæ–°æ ‡é¢˜çº§åˆ«æ›´é«˜æˆ–ç›¸ç­‰ï¼Œå¼¹å‡ºæ ˆé¡¶
                while header_stack and header_stack[-1].level >= new_level:
                    header_stack.pop()

                # Push: å…¥æ ˆå½“å‰æ ‡é¢˜
                header_stack.append(HeaderNode(level=new_level, title=new_title))

                # æŠŠæ ‡é¢˜ä¹ŸåŠ åˆ°æ­£æ–‡é‡Œï¼Œä¿è¯è¯­ä¹‰è¿è´¯ (è¿™å¯¹ Embedding å¾ˆæœ‰å¸®åŠ©)
                content_buffer.append(f"ã€{new_title}ã€‘")

            else:
                if line.strip():  # å¿½ç•¥çº¯ç©ºè¡Œ
                    content_buffer.append(line)

        # 3. å¤„ç†æœ€åä¸€æ®µé—ç•™æ–‡æœ¬
        if content_buffer:
            self._save_chunk(chunks, header_stack, content_buffer, source_name)

        return chunks

    def _save_chunk(self, chunks, header_stack, content_buffer, source_name):
        """è¾…åŠ©å‡½æ•°ï¼šæ‰“åŒ…æ•°æ®"""
        full_text = "\n".join(content_buffer).strip()
        if not full_text:
            return

        # ç”Ÿæˆé¢åŒ…å±‘è·¯å¾„: "Root > 1. é¡¹ç›®èƒŒæ™¯ > 1.1 ç°çŠ¶"
        headers_list = [h.title for h in header_stack]
        current_path = " > ".join(headers_list) or "Root"

        chunk_obj = MarkdownChunk(
            id=str(uuid.uuid4()),
            content=full_text,
            metadata={
                "source": source_name,
                "path": current_path,
                "headers": headers_list,  # âœ… å¿…é¡»åŠ è¿™ä¸ªï¼Œç»™ Neo4j ç”¨
                "level": len(header_stack)
            }
        )
        chunks.append(asdict(chunk_obj))


def process_all_markdowns():
    splitter = MarkdownContextSplitter()
    all_chunks = []

    # æ‰«æ Markdown æ–‡ä»¶
    md_files = list(INPUT_DIR.glob("*.md"))
    if not md_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {INPUT_DIR} æ²¡æ‰¾åˆ° .md æ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œ pdf_parser.py")
        return

    print(f"ğŸš€ [AST Logic] æ­£åœ¨é€’å½’åˆ‡åˆ† {len(md_files)} ä¸ª Markdown æ–‡ä»¶...")

    for md_file in md_files:
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()

        file_chunks = splitter.split_text(content, source_name=md_file.name)
        all_chunks.extend(file_chunks)
        print(f"  -> {md_file.name}: ç”Ÿæˆ {len(file_chunks)} ä¸ªä¸Šä¸‹æ–‡è¯­ä¹‰å—")

    # ä¿å­˜ç»“æœ
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for chunk in all_chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + "\n")

    print(f"âœ… åˆ‡åˆ†å®Œæˆï¼å·²ä¿å­˜è‡³: {OUTPUT_FILE.name}")


if __name__ == "__main__":
    process_all_markdowns()