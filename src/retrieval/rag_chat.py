import chromadb
from chromadb.utils import embedding_functions
from neo4j import GraphDatabase
from openai import OpenAI
from pathlib import Path
import re

# === âš™ï¸ å…¨å±€é…ç½® ===
BASE_DIR = Path(__file__).resolve().parent.parent.parent
CHROMA_PATH = BASE_DIR / "data" / "chroma_db"

NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "zyyzdy0518"  # âœ… ä½ çš„å¯†ç 

OLLAMA_API_KEY = "ollama"
OLLAMA_URL = "http://localhost:11434/v1"
MODEL_NAME = "deepseek-r1:1.5b"  # âœ… ä½ çš„æ¨¡å‹


class HybridRAG:
    def __init__(self):
        # 1. è¿æ¥ Chroma
        self.chroma_client = chromadb.PersistentClient(path=str(CHROMA_PATH))
        self.emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        self.collection = self.chroma_client.get_collection(
            name="proposal_knowledge_base",
            embedding_function=self.emb_fn
        )

        # 2. è¿æ¥ Neo4j
        self.driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

        # 3. è¿æ¥ DeepSeek
        self.llm = OpenAI(api_key=OLLAMA_API_KEY, base_url=OLLAMA_URL)

    def close(self):
        self.driver.close()

    def clean_think(self, text):
        return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()

    def get_graph_context(self, chunk_ids):
        """æ ¸å¿ƒï¼šå» Neo4j é‡ŒæŸ¥"""
        if not chunk_ids: return []

        # âœ… ä¿®æ­£ç‚¹ï¼šå°† CONTAINS æ”¹ä¸º HAS_CHUNK
        cypher = """
        MATCH (c:Chunk)
        WHERE c.id IN $ids

        // 1. æ‰¾çˆ¶ç« èŠ‚ (Structure) - æ³¨æ„æ–¹å‘æ˜¯ c <-[HAS_CHUNK]- s
        OPTIONAL MATCH (c)<-[:HAS_CHUNK]-(s:Section)

        // 2. æ‰¾å…³è”å®ä½“ (Semantics)
        OPTIONAL MATCH (c)-[:HAS_ENTITY]->(e:Entity)

        RETURN 
            c.id AS id,
            s.title AS section,
            collect(e.name) AS entities
        """

        enriched_info = {}
        with self.driver.session() as session:
            result = session.run(cypher, ids=chunk_ids)
            for record in result:
                c_id = record["id"]
                enriched_info[c_id] = {
                    "section": record["section"] or "Unknown Section",
                    "entities": record["entities"]
                }
        return enriched_info

    def chat(self, query):
        print(f"\nğŸ—£ï¸ User: {query}")
        print("-" * 40)

        # Step 1: å‘é‡æ£€ç´¢
        print("ğŸ” 1. ChromaDB: æ­£åœ¨å®šä½åˆ‡ç‰‡...")
        results = self.collection.query(query_texts=[query], n_results=3)

        docs = results['documents'][0]
        ids = results['ids'][0]

        # Step 2: å›¾è°±å¢å¼º
        print(f"ğŸ•¸ï¸ 2. Neo4j: æ­£åœ¨æ‰©å±•ä¸Šä¸‹æ–‡ (IDs: {ids})...")
        graph_data = self.get_graph_context(ids)

        # ç»„è£… Prompt
        context_parts = []
        for i, doc in enumerate(docs):
            c_id = ids[i]
            # è·å–å›¾è°±ä¿¡æ¯
            g_info = graph_data.get(c_id, {"section": "N/A", "entities": []})

            # âœ… è¿™é‡Œçš„ entities åº”è¯¥ä¸ä¸ºç©ºäº†
            entities_str = ', '.join(g_info['entities']) if g_info['entities'] else "æ— å…³è”å®ä½“"

            snippet = f"""
            [å‚è€ƒç‰‡æ®µ {i + 1}]
            - æ¥æºç« èŠ‚: {g_info['section']}
            - çŸ¥è¯†å›¾è°±å…³è”: {entities_str}
            - æ­£æ–‡å†…å®¹: {doc}
            """
            context_parts.append(snippet)

        full_context = "\n".join(context_parts)

        # Step 3: DeepSeek ç”Ÿæˆ
        print("ğŸ¤– 3. DeepSeek: æ­£åœ¨æ€è€ƒ...")

        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®ç”³æŠ¥åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ã€å›¾è°±å¢å¼ºä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ã€‚"
        user_prompt = f"ã€ä¸Šä¸‹æ–‡ã€‘ï¼š\n{full_context}\n\nã€é—®é¢˜ã€‘ï¼š{query}"

        try:
            resp = self.llm.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3
            )
            answer = self.clean_think(resp.choices[0].message.content)

            print("-" * 40)
            print(f"ğŸ§  AI Answer:\n{answer}")
            print("-" * 40)

        except Exception as e:
            print(f"âŒ DeepSeek è°ƒç”¨å¤±è´¥: {e}")
            print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ Ollama æ˜¯å¦å¼€å¯ (ollama serve)")


if __name__ == "__main__":
    bot = HybridRAG()
    bot.chat("Transformer çš„æ ¸å¿ƒæœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå’Œ RNN æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ")
    bot.close()