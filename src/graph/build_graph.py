import os
import logging
from pathlib import Path
from tqdm import tqdm
from neo4j import GraphDatabase
from langchain_text_splitters import MarkdownHeaderTextSplitter
# âœ… ä½¿ç”¨ HuggingFace æœ¬åœ°è¿è¡Œå‘é‡æ¨¡å‹ (ä¸ä¾èµ– Ollamaï¼Œæ›´ç¨³å®š)
from langchain_huggingface import HuggingFaceEmbeddings

# === é…ç½® ===
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "zyyzdy0518"

# è·¯å¾„
CURRENT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = CURRENT_DIR.parent.parent
INPUT_DIR = PROJECT_ROOT / "data" / "processed"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')


class KnowledgeGraphBuilder:
    def __init__(self):
        self.driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

        #  è¿™é‡Œä¼šè‡ªåŠ¨ä¸‹è½½ BAAI/bge-small-zh-v1.5
        # è¿™æ˜¯ä¸€ä¸ªä¸“é—¨çš„å‘é‡æ¨¡å‹ï¼Œè™½å°(100MB)ä½†ä¸­æ–‡æ£€ç´¢èƒ½åŠ›æå¼ºï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œ
        print("[ç³»ç»Ÿ] åŠ è½½æœ¬åœ° Embedding æ¨¡å‹ (BAAI/bge-small-zh-v1.5)...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-zh-v1.5"
        )
        print("æ¨¡å‹åŠ è½½å®Œæ¯•")

    def close(self):
        self.driver.close()

    def create_vector_index(self):
        with self.driver.session() as session:
            print("é‡å»ºç´¢å¼• (512ç»´)...")
            session.run("DROP INDEX vector_index IF EXISTS")
            # bge-small-zh æ˜¯ 512 ç»´
            session.run("""
            CREATE VECTOR INDEX vector_index IF NOT EXISTS
            FOR (c:Chunk) ON (c.embedding)
            OPTIONS {indexConfig: {`vector.dimensions`: 512, `vector.similarity_function`: 'cosine'}}
            """)

    def build(self):
        if not INPUT_DIR.exists(): return
        md_files = list(INPUT_DIR.glob("*.md"))
        # æŒ‰ç…§ Markdown æ ‡é¢˜åˆ‡åˆ†
        splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[("#", "h1"), ("##", "h2")])

        with self.driver.session() as session:
            for md_file in tqdm(md_files, desc="æœ¬åœ°å…¥åº“ä¸­"):
                file_name = md_file.stem
                if session.run("MATCH (d:Document {name: $name}) RETURN count(d)", name=file_name).single()[0] > 0:
                    continue

                content = md_file.read_text(encoding="utf-8")
                splits = splitter.split_text(content)
                session.run("MERGE (d:Document {name: $name})", name=file_name)

                for i, split in enumerate(splits):
                    # æœ¬åœ° CPU ç”Ÿæˆå‘é‡
                    vector = self.embeddings.embed_query(split.page_content)

                    session.run("""
                        MATCH (d:Document {name: $doc_name})
                        MERGE (c:Chunk {id: $id})
                        SET c.text = $text, c.embedding = $vector
                        MERGE (d)-[:HAS_CHUNK]->(c)
                    """, doc_name=file_name, id=f"{file_name}_{i}", text=split.page_content, vector=vector)


if __name__ == "__main__":
    builder = KnowledgeGraphBuilder()
    builder.create_vector_index()  # å¿…é¡»è·‘ï¼Œé‡ç½®ç´¢å¼•
    builder.build()
    builder.close()
    print("ğŸ‰ æœ¬åœ°å…¥åº“å®Œæˆï¼")