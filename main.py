import json
from pathlib import Path
# å¯¼å…¥æˆ‘ä»¬åˆšå†™çš„åˆ‡åˆ†å™¨
from src.processing.markdown_splitter import MarkdownContextSplitter

# è·¯å¾„é…ç½®
BASE_DIR = Path(__file__).parent
PROCESSED_DIR = BASE_DIR / "data" / "processed"
OUTPUT_FILE = PROCESSED_DIR / "chunks_cache.jsonl"


def main():
    # 1. æ‰«ææ‰€æœ‰çš„ .md æ–‡ä»¶ (ç”± pdf_parser.py ç”Ÿæˆ)
    md_files = list(PROCESSED_DIR.glob("*.md"))

    if not md_files:
        print("âŒ æ²¡æ‰¾åˆ° Markdown æ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œ pdf_parser.py è§£æ PDFã€‚")
        return

    print(f"ğŸ“‚ æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡æ¡£ï¼Œå‡†å¤‡åˆ‡åˆ†...")

    splitter = MarkdownContextSplitter()
    total_chunks = 0

    # 2. æ‰“å¼€è¾“å‡ºæ–‡ä»¶ (JSONL æ ¼å¼)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f_out:
        for md_file in md_files:
            print(f"âš¡ Processing: {md_file.name}")

            with open(md_file, "r", encoding="utf-8") as f_in:
                text = f_in.read()

            # === è°ƒç”¨æ ¸å¿ƒåˆ‡åˆ†é€»è¾‘ ===
            chunks = splitter.split_text(text, source_name=md_file.name)

            # å†™å…¥æ–‡ä»¶
            for chunk in chunks:
                f_out.write(json.dumps(chunk, ensure_ascii=False) + "\n")

            print(f"   -> ç”Ÿæˆ {len(chunks)} ä¸ªåˆ‡ç‰‡")
            total_chunks += len(chunks)

    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±ç”Ÿæˆ: {total_chunks} ä¸ªçŸ¥è¯†å—")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()